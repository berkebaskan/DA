{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8449565,"sourceType":"datasetVersion","datasetId":5035256}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Credit Card Approval Determination\n\nCredit card approval determination is a critical process for financial institutions, where they evaluate potential customers' creditworthiness to decide whether to approve or reject credit card applications. This process involves analyzing various attributes of applicants to assess their ability to repay the credit and manage their financial responsibilities. The goal is to minimize the risk of defaults while ensuring that deserving applicants have access to credit.\n\n#### Key Attributes in Credit Card Approval\n\nHere are the key attributes commonly used in the credit card approval process and their significance:\n\n- **ID**: A unique identifier for each applicant. It ensures that each record is distinct and traceable.\n  \n- **Gender**: The applicant's gender. This demographic information can sometimes be used in statistical analyses to understand trends, although it should be handled carefully to avoid discrimination.\n\n- **Own_car**: Indicates whether the applicant owns a car. Car ownership can be a proxy for financial stability and asset ownership.\n\n- **Own_property**: Indicates whether the applicant owns property. Property ownership is often associated with financial stability and creditworthiness.\n\n- **Work_phone**: Whether the applicant has a work phone. This can be an indicator of stable employment.\n\n- **Phone**: Whether the applicant has a personal phone. Having a phone can be a basic requirement for communication and contactability.\n\n- **Email**: Whether the applicant has an email address. This is important for communication and can also indicate digital literacy.\n\n- **Unemployed**: Indicates whether the applicant is currently unemployed. Employment status is a crucial factor in assessing income stability.\n\n- **Num_children**: The number of children the applicant has. This can affect the applicant's financial responsibilities and disposable income.\n\n- **Num_family**: The total number of family members. Larger family size might indicate higher financial obligations.\n\n- **Account_length**: The length of time the applicant has had an account with the bank. Longer account history can be a sign of stability and reliability.\n\n- **Total_income**: The total income of the applicant. Higher income generally increases the likelihood of credit approval as it indicates the ability to repay.\n\n- **Age**: The age of the applicant. Age can correlate with financial experience and stability.\n\n- **Years_employed**: The number of years the applicant has been employed. Longer employment history usually suggests job stability and steady income.\n\n- **Income_type**: The type of income (e.g., salary, business income). Different income types can have varying levels of reliability and stability.\n\n- **Education_type**: The applicant's level of education. Higher education levels can correlate with better job prospects and financial literacy.\n\n- **Family_status**: The applicant's marital status (e.g., single, married). Family status can influence financial responsibilities and stability.\n\n- **Housing_type**: The type of housing (e.g., rented, owned). Owning a home can be a sign of financial stability.\n\n- **Occupation_type**: The applicant's occupation. Certain occupations might be seen as more stable or higher earning, influencing creditworthiness.\n\n- **Target**: The target variable indicating whether the credit card application was approved or not. This is the outcome we aim to predict based on the other attributes.\n\nThese attributes collectively provide a comprehensive profile of the applicant, enabling financial institutions to make informed decisions about credit card approvals. By analyzing these factors, banks can assess the risk associated with each applicant and ensure that credit is extended to individuals who are likely to manage their credit responsibly.","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:56:50.275322Z","iopub.execute_input":"2024-05-30T18:56:50.275751Z","iopub.status.idle":"2024-05-30T18:56:50.296486Z","shell.execute_reply.started":"2024-05-30T18:56:50.275721Z","shell.execute_reply":"2024-05-30T18:56:50.294971Z"}}},{"cell_type":"markdown","source":"### Introduction\n\nIn this notebook, we aim to address the presence of outliers in our dataset, which can potentially skew the results of our machine learning models and lead to inaccurate predictions. Here's a detailed overview of the steps we will take to handle outliers and improve the quality of our data:\n\n- **Understanding Outliers**:\n  - Outliers are extreme values that deviate significantly from the majority of the data.\n  - They can arise from various factors such as measurement errors, data entry errors, or genuine variability in the data.\n  - Outliers can disproportionately impact statistical analyses and machine learning models, often leading to biased estimates and reduced model performance.\n\n- **Importance of Handling Outliers**:\n  - To ensure robust model performance and reliable predictions, it is crucial to identify and handle outliers appropriately.\n  - Removing outliers helps in reducing noise and variability in the data, enhancing the accuracy and generalizability of our models.\n\n- **Steps in This Notebook**:\n\n  1. **Load the Dataset**:\n     - Import necessary libraries.\n     - Load the dataset into a pandas DataFrame for analysis and manipulation.\n\n  2. **Inspect the Data**:\n     - Get a preliminary understanding of the dataset by examining its structure, summary statistics, and the distribution of target variables.\n     - Identify the numerical columns in the dataset that will be analyzed for outliers.\n\n  3. **Identify Outliers**:\n     - For each numerical column, calculate the 99th percentile value to establish the threshold for identifying outliers.\n     - Count the number of data points that exceed this 99th percentile threshold for each numerical column.\n     - Print the count of outliers for each numerical column to understand the extent of outliers in the dataset.\n\n  4. **Remove Outliers**:\n     - Filter the DataFrame to exclude data points that exceed the 99th percentile threshold for each numerical column.\n     - This step ensures that the dataset is cleaned by removing extreme values that could distort the analysis and model training.\n\n  5. **Evaluate the Impact of Removing Outliers**:\n     - Compare the shape of the original and cleaned datasets to understand the reduction in data size.\n     - Perform summary statistics and visualizations on the cleaned dataset to ensure data integrity and distribution.\n\n  6. **Scale the Data**:\n     - Apply feature scaling using StandardScaler to normalize the numerical features, ensuring that they have a mean of 0 and a standard deviation of 1.\n     - This step is essential for improving the performance and convergence speed of many machine learning algorithms.\n\n  7. **Handle Class Imbalance with SMOTE**:\n     - Apply the Synthetic Minority Over-sampling Technique (SMOTE) to balance the classes in the target variable.\n     - SMOTE generates synthetic samples for the minority class, reducing the bias towards the majority class and improving model performance.\n\n  8. **Train the Initial Model**:\n     - Train a Random Forest classifier on the balanced and scaled dataset to identify feature importance.\n     - Extract and visualize the feature importances to understand which features contribute the most to the model's predictions.\n\n  9. **Select Top 10 Features**:\n     - Based on the feature importances, select the top 10 most important features.\n     - Reduce the dataset to include only these top 10 features for further analysis and model training.\n\n 10. **Hyperparameter Tuning**:\n     - Perform hyperparameter tuning using GridSearchCV to find the best combination of hyperparameters for the Random Forest classifier.\n     - This step involves testing various combinations of parameters to optimize the model's performance.\n\n 11. **Train the Final Model**:\n     - Train the Random Forest classifier using the best parameters obtained from the hyperparameter tuning on the top 10 features.\n     - Evaluate the final model using accuracy, classification report, and confusion matrix to assess its performance.\n\nBy performing these steps, we aim to enhance the quality of our data, leading to better model training and more accurate predictions. This approach ensures that our models are trained on data that truly represents the underlying patterns and relationships, resulting in reliable and effective machine learning models.","metadata":{}},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:39:42.564942Z","iopub.execute_input":"2024-05-30T18:39:42.565595Z","iopub.status.idle":"2024-05-30T18:39:42.573054Z","shell.execute_reply.started":"2024-05-30T18:39:42.565563Z","shell.execute_reply":"2024-05-30T18:39:42.571491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/credit-card-eligibility-data-determining-factors/dataset.csv')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:39:42.594671Z","iopub.execute_input":"2024-05-30T18:39:42.595409Z","iopub.status.idle":"2024-05-30T18:39:42.653496Z","shell.execute_reply.started":"2024-05-30T18:39:42.595368Z","shell.execute_reply":"2024-05-30T18:39:42.652248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Basic Checks ","metadata":{}},{"cell_type":"code","source":"#checking Missing Values\ndef missing_values(data):\n    missing_vals = data.isnull().sum()\n    print(\"missing values are :\\n\", missing_vals)\nmissing_values(data)    ","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:39:42.655564Z","iopub.execute_input":"2024-05-30T18:39:42.655934Z","iopub.status.idle":"2024-05-30T18:39:42.672641Z","shell.execute_reply.started":"2024-05-30T18:39:42.655904Z","shell.execute_reply":"2024-05-30T18:39:42.671103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking Duplicated Rows\ndef duplicated_vals(data):\n    duplicated_rows = data.duplicated().sum()\n    print(\"duplicated rows are: \\n\",duplicated_rows)\nduplicated_vals(data)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:39:42.674704Z","iopub.execute_input":"2024-05-30T18:39:42.675082Z","iopub.status.idle":"2024-05-30T18:39:42.698049Z","shell.execute_reply.started":"2024-05-30T18:39:42.675049Z","shell.execute_reply":"2024-05-30T18:39:42.696791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:39:42.69963Z","iopub.execute_input":"2024-05-30T18:39:42.700494Z","iopub.status.idle":"2024-05-30T18:39:42.762561Z","shell.execute_reply.started":"2024-05-30T18:39:42.700462Z","shell.execute_reply":"2024-05-30T18:39:42.761328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:39:42.765697Z","iopub.execute_input":"2024-05-30T18:39:42.76607Z","iopub.status.idle":"2024-05-30T18:39:42.791406Z","shell.execute_reply.started":"2024-05-30T18:39:42.766037Z","shell.execute_reply":"2024-05-30T18:39:42.790253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()\nprint(\"categorical columns are : \",categorical_cols)\nfor i in data[categorical_cols]:\n    print(\"\\n\")\n    print(f\"Distinct Value Counts of {i} :\\n\",data[i].value_counts())\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:39:42.7927Z","iopub.execute_input":"2024-05-30T18:39:42.793044Z","iopub.status.idle":"2024-05-30T18:39:42.819543Z","shell.execute_reply.started":"2024-05-30T18:39:42.793014Z","shell.execute_reply":"2024-05-30T18:39:42.817855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Function to count outliers based on the 99th percentile\ndef count_outliers(df):\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    outliers = {}\n    for col in numeric_cols:\n        upper_limit = df[col].quantile(0.99)\n        outliers[col] = (df[col] > upper_limit).sum()\n    return outliers\n\n# Count outliers\noutliers_count = count_outliers(data.drop(columns = 'ID'))\n\n# Print the count of outliers for each numerical column\nprint(\"Outliers count based on the 99th percentile:\")\nfor col, count in outliers_count.items():\n    print(f\"{col}: {count}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:39:42.821463Z","iopub.execute_input":"2024-05-30T18:39:42.822154Z","iopub.status.idle":"2024-05-30T18:39:42.85353Z","shell.execute_reply.started":"2024-05-30T18:39:42.822121Z","shell.execute_reply":"2024-05-30T18:39:42.852344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to remove outliers based on the 99th percentile\ndef remove_outliers(df):\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    for col in numeric_cols:\n        upper_limit = df[col].quantile(0.99)\n        df = df[df[col] <= upper_limit]\n    return df\n\n# Remove outliers\ndf_cleaned = remove_outliers(data.drop(columns = 'ID'))\n\n# Check the shape of the cleaned dataset\nprint(\"Original dataset shape:\", data.drop(columns = 'ID').shape)\nprint(\"Cleaned dataset shape:\", df_cleaned.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:41:19.343094Z","iopub.execute_input":"2024-05-30T18:41:19.343505Z","iopub.status.idle":"2024-05-30T18:41:19.386747Z","shell.execute_reply.started":"2024-05-30T18:41:19.343474Z","shell.execute_reply":"2024-05-30T18:41:19.385319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df_cleaned","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:41:25.013025Z","iopub.execute_input":"2024-05-30T18:41:25.013971Z","iopub.status.idle":"2024-05-30T18:41:25.019542Z","shell.execute_reply.started":"2024-05-30T18:41:25.013926Z","shell.execute_reply":"2024-05-30T18:41:25.017806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# the distribution of individuals based on gender\ncolors = ['#1f77b4', '#ff7f0e']\nplt.figure(figsize=(8, 6))\n# Calculate the proportion of males and females\ngender_counts = data['Gender'].value_counts()\ntotal_individuals = len(data)\nmale_count = gender_counts[1]\nfemale_count = gender_counts[0]\nmale_proportion = male_count / total_individuals\nfemale_proportion = female_count / total_individuals\nsns.barplot(x=['Male', 'Female'], y=[male_count, female_count], palette=colors)\nfor index, value in enumerate([male_count, female_count]):\n    plt.text(index, value + 5, str(value), ha='center', va='bottom', fontsize=12)\nplt.xlabel('Gender', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.title('Distribution of Individuals based on Gender', fontsize=14)\nsns.despine()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:41:26.370109Z","iopub.execute_input":"2024-05-30T18:41:26.370526Z","iopub.status.idle":"2024-05-30T18:41:26.692182Z","shell.execute_reply.started":"2024-05-30T18:41:26.370494Z","shell.execute_reply":"2024-05-30T18:41:26.690897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = data\n\n# Set the aesthetic style of the plots\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(12, 6))\n\n# Plot the distribution of the target variable\nplt.subplot(2, 3, 1)\nsns.countplot(data=df, x='Target', palette='viridis')\nplt.title('Distribution of Target Variable')\n\n# Plot the distribution of numeric variables\nplt.subplot(2, 3, 2)\nsns.histplot(df['Age'], kde=True, bins=30, color='blue')\nplt.title('Distribution of Age')\n\nplt.subplot(2, 3, 3)\nsns.histplot(df['Total_income'], kde=True, bins=30, color='green')\nplt.title('Distribution of Total Income')\n\n# Pairplot of selected features\nselected_features = ['Age', 'Total_income', 'Years_employed', 'Target']\nsns.pairplot(df[selected_features], hue='Target', palette='coolwarm')\nplt.suptitle('Pairplot of Selected Features', y=1.02)\n\n# Boxplot of Total Income by Target\nplt.figure(figsize=(10, 5))\nsns.boxplot(x='Target', y='Total_income', data=df, palette='coolwarm')\nplt.title('Total Income by Target')\n\n# Boxplot of Age by Target\nplt.figure(figsize=(10, 5))\nsns.boxplot(x='Target', y='Age', data=df, palette='coolwarm')\nplt.title('Age by Target')\n\n# Countplot of categorical variables\nplt.figure(figsize=(15, 10))\n\nplt.subplot(2, 3, 1)\nsns.countplot(data=df, x='Gender', hue='Target', palette='viridis')\nplt.title('Gender by Target')\n\nplt.subplot(2, 3, 2)\nsns.countplot(data=df, x='Own_car', hue='Target', palette='viridis')\nplt.title('Own Car by Target')\n\nplt.subplot(2, 3, 3)\nsns.countplot(data=df, x='Own_property', hue='Target', palette='viridis')\nplt.title('Own Property by Target')\n\nplt.subplot(2, 3, 4)\nsns.countplot(data=df, x='Work_phone', hue='Target', palette='viridis')\nplt.title('Work Phone by Target')\n\nplt.subplot(2, 3, 5)\nsns.countplot(data=df, x='Phone', hue='Target', palette='viridis')\nplt.title('Phone by Target')\n\nplt.subplot(2, 3, 6)\nsns.countplot(data=df, x='Email', hue='Target', palette='viridis')\nplt.title('Email by Target')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:41:26.858572Z","iopub.execute_input":"2024-05-30T18:41:26.858939Z","iopub.status.idle":"2024-05-30T18:41:40.274406Z","shell.execute_reply.started":"2024-05-30T18:41:26.85891Z","shell.execute_reply":"2024-05-30T18:41:40.272803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the correlation matrix\ncorr = data.select_dtypes(include=['int64', 'float64']).corr()\ncorr","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:41:40.277106Z","iopub.execute_input":"2024-05-30T18:41:40.277703Z","iopub.status.idle":"2024-05-30T18:41:40.324546Z","shell.execute_reply.started":"2024-05-30T18:41:40.277666Z","shell.execute_reply":"2024-05-30T18:41:40.323053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"# Step 1: Import necessary libraries\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom imblearn.over_sampling import SMOTE\n\ndf = data.drop(columns = ['ID'])\ndf = df.select_dtypes(include=['int64', 'float64'])\n\n# Step 3: Split the dataset into features (X) and target (y)\nX = df.drop('Target', axis=1)\ny = df['Target']","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:41:40.32627Z","iopub.execute_input":"2024-05-30T18:41:40.326617Z","iopub.status.idle":"2024-05-30T18:41:41.127107Z","shell.execute_reply.started":"2024-05-30T18:41:40.326588Z","shell.execute_reply":"2024-05-30T18:41:41.125759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 4: Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:41:41.129742Z","iopub.execute_input":"2024-05-30T18:41:41.130369Z","iopub.status.idle":"2024-05-30T18:41:41.142475Z","shell.execute_reply.started":"2024-05-30T18:41:41.130333Z","shell.execute_reply":"2024-05-30T18:41:41.14058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 5: Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:41:41.144982Z","iopub.execute_input":"2024-05-30T18:41:41.145906Z","iopub.status.idle":"2024-05-30T18:41:41.162381Z","shell.execute_reply.started":"2024-05-30T18:41:41.14586Z","shell.execute_reply":"2024-05-30T18:41:41.16104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 6: Apply SMOTE to the training data\nsmote = SMOTE(random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:41:41.164267Z","iopub.execute_input":"2024-05-30T18:41:41.164736Z","iopub.status.idle":"2024-05-30T18:41:41.208717Z","shell.execute_reply.started":"2024-05-30T18:41:41.164697Z","shell.execute_reply":"2024-05-30T18:41:41.207203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 7: Train the Random Forest classifier to find feature importance\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_classifier.fit(X_train_smote, y_train_smote)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:41:41.210387Z","iopub.execute_input":"2024-05-30T18:41:41.210858Z","iopub.status.idle":"2024-05-30T18:41:43.798772Z","shell.execute_reply.started":"2024-05-30T18:41:41.210816Z","shell.execute_reply":"2024-05-30T18:41:43.797283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 8: Extract feature importance\nfeature_importances = rf_classifier.feature_importances_\nfeatures = X.columns\nimportance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\nimportance_df = importance_df.sort_values(by='Importance', ascending=False)\n\n# Plot feature importance\nplt.figure(figsize=(12, 8))\nsns.barplot(x='Importance', y='Feature', data=importance_df)\nplt.title('Feature Importance')\nplt.show()\n\n# Step 9: Analyze the top features\ntop_features = importance_df.head(10)\nprint(top_features)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:41:43.800564Z","iopub.execute_input":"2024-05-30T18:41:43.801093Z","iopub.status.idle":"2024-05-30T18:41:44.378847Z","shell.execute_reply.started":"2024-05-30T18:41:43.801Z","shell.execute_reply":"2024-05-30T18:41:44.377493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select top 10 important features\ntop_10_features = importance_df.head(10)['Feature'].tolist()\n\n# Step 9: Reduce the dataset to top 10 features\nX_train_top10 = X_train[top_10_features]\nX_test_top10 = X_test[top_10_features]","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:41:44.380455Z","iopub.execute_input":"2024-05-30T18:41:44.380913Z","iopub.status.idle":"2024-05-30T18:41:44.391508Z","shell.execute_reply.started":"2024-05-30T18:41:44.38087Z","shell.execute_reply":"2024-05-30T18:41:44.390194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 10: Scale the top 10 features\nX_train_top10_scaled = scaler.fit_transform(X_train_top10)\nX_test_top10_scaled = scaler.transform(X_test_top10)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:41:44.395725Z","iopub.execute_input":"2024-05-30T18:41:44.396084Z","iopub.status.idle":"2024-05-30T18:41:44.411643Z","shell.execute_reply.started":"2024-05-30T18:41:44.396056Z","shell.execute_reply":"2024-05-30T18:41:44.410421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 11: Apply SMOTE to the top 10 features\nX_train_top10_smote, y_train_top10_smote = smote.fit_resample(X_train_top10_scaled, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:41:44.413262Z","iopub.execute_input":"2024-05-30T18:41:44.414433Z","iopub.status.idle":"2024-05-30T18:41:44.446984Z","shell.execute_reply.started":"2024-05-30T18:41:44.41439Z","shell.execute_reply":"2024-05-30T18:41:44.445734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 12: Hyperparameter tuning using GridSearchCV\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\ngrid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42),\n                           param_grid=param_grid,\n                           cv=5, n_jobs=-1, verbose=2)\n\ngrid_search.fit(X_train_top10_smote, y_train_top10_smote)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:41:44.448721Z","iopub.execute_input":"2024-05-30T18:41:44.449091Z","iopub.status.idle":"2024-05-30T18:55:25.287302Z","shell.execute_reply.started":"2024-05-30T18:41:44.449061Z","shell.execute_reply":"2024-05-30T18:55:25.28626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 13: Train the Random Forest classifier with best parameters\nbest_params = grid_search.best_params_\nrf_classifier_tuned = RandomForestClassifier(**best_params, random_state=42)\nrf_classifier_tuned.fit(X_train_top10_smote, y_train_top10_smote)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:55:25.289146Z","iopub.execute_input":"2024-05-30T18:55:25.289836Z","iopub.status.idle":"2024-05-30T18:55:34.371442Z","shell.execute_reply.started":"2024-05-30T18:55:25.2898Z","shell.execute_reply":"2024-05-30T18:55:34.370203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 14: Make predictions using the tuned model\ny_pred_tuned = rf_classifier_tuned.predict(X_test_top10_scaled)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:55:34.372895Z","iopub.execute_input":"2024-05-30T18:55:34.373281Z","iopub.status.idle":"2024-05-30T18:55:34.552201Z","shell.execute_reply.started":"2024-05-30T18:55:34.373252Z","shell.execute_reply":"2024-05-30T18:55:34.551014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"# Step 15: Evaluate the tuned model\naccuracy_tuned = accuracy_score(y_test, y_pred_tuned)\nprint(f'Accuracy with Tuned Model: {accuracy_tuned:.2f}')\nprint('Classification Report with Tuned Model:')\nprint(classification_report(y_test, y_pred_tuned))\nprint('Confusion Matrix with Tuned Model:')\nprint(confusion_matrix(y_test, y_pred_tuned))","metadata":{"execution":{"iopub.status.busy":"2024-05-30T18:55:34.553751Z","iopub.execute_input":"2024-05-30T18:55:34.554188Z","iopub.status.idle":"2024-05-30T18:55:34.578377Z","shell.execute_reply.started":"2024-05-30T18:55:34.55416Z","shell.execute_reply":"2024-05-30T18:55:34.577071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}